# Computer Vision for Surgical Tools â€” 2D Pose Estimation

<p align="center">
  <b>Phase 1:</b> Synthetic Data &nbsp;â€¢&nbsp;
  <b>Phase 2:</b> Train & Evaluate &nbsp;â€¢&nbsp;
  <b>Phase 3:</b> Unsupervised Refinement on Real Video
</p>

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/PyTorch-âœ”-orange.svg)]()
[![BlenderProc](https://img.shields.io/badge/BlenderProc-âœ”-brightgreen.svg)]()
[![YOLO](https://img.shields.io/badge/YOLOv8-pose-success.svg)]()

---

## Project Intro

This repository develops a **2D pose estimation system** for surgical instruments using synthetic data first, then adapts to real, unlabeled videos.

- **Goal:** Detect per-tool keypoints and poses in 2D images/videos.
- **Key challenges:** Occlusion, articulation, domain gap, and real-time constraints.
- **Approach:** Synthetic data âœ train on synthetic âœ refine on unlabeled real video (self-training / pseudo-labels).

---
The project was splitted into 3 phases (parts):
1. Phase 1: dataset generation, domain gap analysis

2. Phase 2: model choice, training details, results

3. Phase 3: refinement strategy, comparison, challenges
---
## â¬‡ï¸ Weights & Artifacts

Phase 2 weights: <link>

Phase 3 weights: <link>

Videos:

results_synthetic_only.mp4

results_refined.mp4

## ğŸ“ Repository Structure 
```text
.
â”œâ”€ Part1/                    
â”‚  â”œâ”€ rendering_{type of rendering}.py     - there are multiple files that create different kinds of rendering
|  â”œâ”€ obj_features.py              - Exploring provided .obj and .mtl files
|  â”œâ”€ overlay_coco_kps.py         - draws key points on images of tools
|  â””â”€ paste_on_random_background.py
â”œâ”€ Part2/                      
â”‚  â”œâ”€ training_model.ipynb 
â”‚  â”œâ”€ data_config.yaml
â”‚  â”œâ”€ coco_to_yolo_pose.py        - converter from coco annotations to yolo
â”‚  â””â”€ runs/                      - logs, checkpoints, metrics (automatic yolo logs) 
â”œâ”€ Part3/                   
â”‚  â”œâ”€ refine.ipynb                  - pseudo-label loop
â”‚  â””â”€ runs_refined/
â”œâ”€ inference/
â”‚  â”œâ”€ predict.py                 - image inference
â”‚  â””â”€ video.py                   - video inference (OpenCV)
â”œâ”€ requirements.txt
â”œâ”€ README.md
â”œâ”€ camera.json       - intrinsics for rendering
â”œâ”€ sample images       - folder with examples of different renderings (.png files)
â””â”€ LICENSE
```
Provided resources (we had on the VM in /datashare/project):

 - 3D CAD models (.obj) of tools (with articulation)

 - Backgrounds: COCO 2017; Textures/HDRI: Polyhaven

 - Unlabeled videos: (4_2_24_A_1.mp4, etc.)


### Saving the datasets
For 2D pose estimation with two object classes (needle holder and tweezers) and synthetic data it ws important to save the dataset in one general, model-agnostic format so we coudld later convert it to whatever a specific model requires.

Our final structure is: 
1. For annotations: COCO format (JSON) â€“ supports multiple objects per image, multiple keypoints per object, segmentation masks if needed.

2. We used next folder structure: (we converted it later to yolo format) 
```
dataset/
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ img_00001.jpg
â”‚   â”‚   â”œâ”€â”€ img_00002.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â”‚
â””â”€â”€ coco_keypoints.json COCO-style annotations for training
```

4. Our annotations were saved in coco-jsonlike this: 
```
{
  "images": [
    {"id": 1, "file_name": "img_00001.jpg", "width": 640, "height": 480}
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "bbox": [x, y, w, h],
      "area": 1234,
      "iscrowd": 0,
      "keypoints": [x1, y1, v1, x2, y2, v2, ...], 
      "num_keypoints": 5
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "needle_holder",
      "keypoints": ["joint","left_handle","left_tip","right_handle","right_tip"],
      "skeleton": [[0, 1], [0, 2], [0, 3], [0, 4]]   
    },
    {
      "id": 2,
      "name": "tweezers",
      "keypoints": ["handle_end","left_arm","left_tip","right_arm","right_tip"],
      "skeleton": [[0, 1], [0, 2], [1, 3], [2, 4]]
    }
  ]
}
```


# Synthetic-data

Installation of BlenderProc: 

```conda create -n synth python=3.10

conda activate synth

pip install blenderproc

blenderproc quickstart
```
